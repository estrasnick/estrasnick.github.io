<!DOCTYPE html>
<html>
  <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
      <title>Evan Strasnick — Boosting Gesture Recognition with an Automatic Gesture Annotation Framework</title>
      <meta name="viewport" content="width=device-width">
      <meta name="description" content="Evan Strasnick — HCI Researcher">

      <!-- syntax highlighting CSS -->
      <link rel="stylesheet" href=/people/estrasni/academic/css/syntax.css>

      <!-- Custom CSS -->
      <link rel="stylesheet" href=/people/estrasni/academic/css/main.css>

      <!-- Responsive CSS -->
      <link rel="stylesheet" href=/people/estrasni/academic/css/responsive.css>

      <!-- Google Fonts -->
      <link rel="preconnect" href="https://fonts.googleapis.com">
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link href="https://fonts.googleapis.com/css2?family=DM+Serif+Display&family=Open+Sans:wght@300&family=Raleway:wght@300;600&display=swap" rel="stylesheet">



  </head>
  <body>
    <div class="site" id="top">
      <div class="sidebar" id="sidebar">
        <div class="header">

          <h1 class="title"><a href=/people/estrasni/academic/><img id="headshot" src=/people/estrasni/academic/images/3_web_cropped.jpg></a></h1>
          <h1 class="title" id="sidebar-name"><a href=/people/estrasni/academic/>Evan<br>Strasnick</a></h1>
        </div>

        <div class="nav" id="div-nav">

          <div class="about" id="about-nav">
            <ul id="navlinks" class="nomarker">
                <li><a href=/people/estrasni/academic/#top id="about-link" class="navlink">About</a></li>
                <li><a href=/people/estrasni/academic/#publications id="research-link" class="navlink">Publications</a></li>
                <li><a href=/people/estrasni/academic/#otherprojects id="proj-link" class="navlink">Other Projects</a></li>
            </ul>
          </div>
        </div>


        <div class="footer" id="about-footer">
          <div class="about" id="about-text">
            <p>PhD: Human-Computer Interaction</p>
            <p>He/him</p>
          </div>
          <div class="about" id="about-links">
            <p><a href=/people/estrasni/academic/CV.pdf>Curriculum Vitae</a></p>
            <p><a href="https://scholar.google.com/citations?user=IpYVaVMAAAAJ&hl=en&oi=ao">Scholar</a></p>
            <p><a href="mailto:estrasnick@gmail.com">Email</a></p>
          </div>
        </div>

      </div>

      <div class="floatbutton">
        <div id="sidebar-button">
          <img id="hamburger" src=/people/estrasni/academic/images/sidebar-button.png>
        </div>
      </div>

      <div class="content" id="home">

        <div id="post-info">
  <div id="cover-photo-container">
    
      <img id="cover-photo" src="/people/estrasni/academic/images/gesture_recognition_q.png">
    
  </div>

  <div id="info-container">
    <h1 id="title">Boosting Gesture Recognition with an Automatic Gesture Annotation Framework</h1>
    <p>Junxiao Shen, Xuhai Xu, Ran Tan, Amy Karlson, Evan Strasnick</p>
    <p>FG, 2024.</p>
    <p><a href="https://brosdocs.net/fg2024/251.pdf">Full Article</a>   </p>
    
  </div>
</div>

<div class="post textblock">
  Training a real-time gesture recognition model heavily relies on annotated data. However, manual data annotation is costly and demands substantial human effort. In order to address this challenge, we propose a framework that can automatically annotate gesture classes and identify their temporal ranges. Our framework consists of two key components: (1) a novel annotation model that leverages the Connectionist Temporal Classification (CTC) loss, and (2) a semi-supervised learning pipeline that enables the model to improve its performance by training on its own predictions, known as pseudo labels. These high-quality pseudo labels can also be used to enhance the accuracy of other downstream gesture recognition models. To evaluate our framework, we conducted experiments using two publicly available gesture datasets. Our ablation study demonstrates that our annotation model design surpasses the baseline in terms of both gesture classification accuracy (3-4% improvement) and localization accuracy (71-75% improvement). Additionally, we illustrate that the pseudo-labeled dataset produced from the proposed framework significantly boosts the accuracy of a pre-trained downstream gesture recognition model by 11-18%. We believe that this annotation framework has immense potential to improve the training of downstream gesture recognition models using unlabeled datasets.
  

</div>



      </div>

    </div>
    <script src=/people/estrasni/academic/scripts/responsive.js type="text/javascript"></script>
  </body>
</html>
